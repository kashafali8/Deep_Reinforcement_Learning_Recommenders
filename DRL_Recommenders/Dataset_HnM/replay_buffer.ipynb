{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMSpUyemxmXrlGNs9dHm37w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kashafali8/Deep_Reinforcement_Learning_Recommenders/blob/main/replay_buffer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kPZeFlLni_E2"
      },
      "outputs": [],
      "source": [
        "from curses import raw\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import argparse\n",
        "import numpy as np\n",
        "# Utility\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_args():\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Generae replay buffer data.\")\n",
        "\n",
        "    parser.add_argument('--data', nargs='?', default='data',\n",
        "                        help='data directory')\n",
        "\n",
        "    parser.add_argument('--state_len', type=int, default=10,\n",
        "                        help='Max state length.')\n",
        "\n",
        "    parser.add_argument('--size', type=int, default=-1,\n",
        "                        help='How many session id will be used to generate train/val/test id.')\n",
        "\n",
        "    parser.add_argument('--random', type=bool, default=True,\n",
        "                        help='Is select session id randomly. If True, then sess_start will be invalided')\n",
        "\n",
        "    parser.add_argument('--seed', type=int, default=1234,\n",
        "                        help='Random seed')\n",
        "\n",
        "    parser.add_argument('--sess_start', type=int, default=0,\n",
        "                        help='Generate from the nth session id')\n",
        "\n",
        "    parser.add_argument('--format', choices=['paper', 'csv'], default='paper',\n",
        "                        help='Output format \"paper\" (paper format) or \"csv\" (csv file)')\n",
        "\n",
        "    #parser.add_argument('--pad', choices=['item_size', '0'], default='paper',\n",
        "    #                    help='Use which mark (\"item_size\", \"0\") as pad')\n",
        "\n",
        "    #parser.add_argument('--train_split', type=float, default='0.8',\n",
        "    #                    help='Split into training and testing data')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def to_pickled_df(data_directory, **kwargs):\n",
        "    for name, df in kwargs.items():\n",
        "        df.to_pickle(os.path.join(data_directory, name + '.df'))\n",
        "\n",
        "\n",
        "def generate_session_id(df):\n",
        "    cus_ids = df['customer_id'].unique()\n",
        "    cus_to_sess = {value: index for index, value in enumerate(cus_ids)}\n",
        "    df['customer_id'] = df['customer_id'].apply(lambda x: cus_to_sess[x])\n",
        "    df = df.rename(columns={'customer_id': 'session_id'})\n",
        "    return df\n",
        "\n",
        "\n",
        "def pad_history(itemlist, length, pad_item):\n",
        "    if len(itemlist)>=length:\n",
        "        return itemlist[-length:]\n",
        "    if len(itemlist)<length:\n",
        "        temp = [pad_item] * (length-len(itemlist))\n",
        "        itemlist.extend(temp)\n",
        "        return itemlist"
      ],
      "metadata": {
        "id": "cuAMUAwKu3I8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "python src/models/gen_replay_buffer.py --size 20000 --format paper --data ../HM_data/\n",
        "python src/models/gen_replay_buffer.py --size 20000 --format csv --data ../HM_data/\n",
        "'''\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # args = parse_args()\n",
        "    # DATA = args.data\n",
        "\n",
        "    args_size = 20000\n",
        "    args_format = \"csv\"\n",
        "    args_data = \"/content/drive/MyDrive/Reinforcement Learning/Dataset_HM/data\"\n",
        "    args_random = True\n",
        "    args_sess_start = 10\n",
        "    args_state_len = 10\n",
        "    args_seed = 1234\n",
        "\n",
        "    # Read all transaction data\n",
        "    print('\\nStart reading all transaction data ...')\n",
        "    start_t = time.time()\n",
        "    raw_data = pd.read_csv(f\"{args_data}/transactions_train.csv\")\n",
        "    print(f'Finish reading in {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_t))}')\n",
        "    \n",
        "    # convert customer_id to session_id; article_id to item_id, t_dat to timestamp\n",
        "    raw_data = generate_session_id(raw_data)\n",
        "    raw_data = raw_data.rename(columns={'article_id':'item_id', 't_dat':'timestamp'})\n",
        "    raw_data = raw_data[raw_data['timestamp'] > '2018-12-31']\n",
        "\n",
        "    # Sample session_ids for processing\n",
        "    session_ids = raw_data['session_id'].unique()\n",
        "    session_size = len(session_ids)\n",
        "    \n",
        "    if args_size == -1:\n",
        "        target_sess_size = session_size\n",
        "    else:\n",
        "        target_sess_size = args_size\n",
        "\n",
        "    if args_random:\n",
        "        np.random.seed(1)\n",
        "        sampled_session_id = np.random.choice(session_ids, size=target_sess_size, replace=False)\n",
        "    else:\n",
        "        sampled_session_id = session_ids[args_sess_start : target_sess_size]\n",
        "\n",
        "    # convert original id to paper style id\n",
        "    item_ids = raw_data['item_id'].unique()\n",
        "    item_size = len(item_ids)\n",
        "    code_to_item = {value: index for index, value in enumerate(item_ids)}\n",
        "    raw_data['item_id'] = raw_data['item_id'].apply(lambda x: code_to_item[x])\n",
        "\n",
        "    # Filter and ave sampled_session.df/csv\n",
        "    print('\\nFilter and save all valid sampled data')\n",
        "    sampled_sessions = raw_data[raw_data['session_id'].isin(sampled_session_id)]\n",
        "\n",
        "    # only keep sessions with length >= 3 <= 50\n",
        "    sampled_sessions['valid_session'] = sampled_sessions.session_id.map(sampled_sessions.groupby('session_id')['item_id'].size() > 2)\n",
        "    sampled_sessions = sampled_sessions.loc[sampled_sessions.valid_session].drop('valid_session', axis=1)\n",
        "    sampled_sessions['valid_session'] = sampled_sessions.session_id.map(sampled_sessions.groupby('session_id')['item_id'].size() < 50)\n",
        "    sampled_sessions = sampled_sessions.loc[sampled_sessions.valid_session].drop('valid_session', axis=1)\n",
        "    # drop unncessary cols\n",
        "    print(sampled_sessions.columns)\n",
        "    sampled_sessions = sampled_sessions.drop(columns=['price', 'sales_channel_id'])\n",
        "    \n",
        "    # all transactions are buy\n",
        "    sampled_sessions.loc[:, 'is_buy'] = 1\n",
        "    # sort by session_if, timestamp\n",
        "    sampled_sessions=sampled_sessions.sort_values(by=['session_id','timestamp'])\n",
        "    sampled_sessions.to_csv(os.path.join(args_data, './sampled_sessions.csv'))\n",
        "    to_pickled_df(args_data, sampled_sessions=sampled_sessions)\n",
        "\n",
        "    # Count popularities of items and save % to pop_dict.txt\n",
        "    print('\\nStart counting popularity ...')\n",
        "    start_t = time.time()\n",
        "    total_actions = sampled_sessions.shape[0]\n",
        "    pop_dict = {}\n",
        "    for idx, row in tqdm(sampled_sessions.iterrows()):\n",
        "        action = row['item_id']\n",
        "        pop_dict[action] = pop_dict[action] + 1 if action in pop_dict else 1\n",
        "    \n",
        "    for action in pop_dict.keys():\n",
        "        pop_dict[action] = float(pop_dict[action])/float(total_actions)\n",
        "    \n",
        "    with open(os.path.join(args_data, 'pop_dict.txt'), 'w') as f:\n",
        "        f.write(str(pop_dict))\n",
        "    print(f'Popularity finished in {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_t))}')\n",
        "\n",
        "\n",
        "    # Split into train, val, test\n",
        "    print('\\nStart spliting into train, val, test data ...')\n",
        "    total_ids = sampled_session_id\n",
        "    np.random.shuffle(total_ids)\n",
        "    train_end_id = int(len(total_ids) * 0.7)\n",
        "    val_end_id   = int(len(total_ids) * 0.9)\n",
        "\n",
        "    train_id = total_ids[:  train_end_id]\n",
        "    val_id   = total_ids[train_end_id : val_end_id]\n",
        "    test_id  = total_ids[val_end_id: ]\n",
        "\n",
        "    train_sessions = sampled_sessions[sampled_sessions['session_id'].isin(train_id)]\n",
        "    val_sessions = sampled_sessions[sampled_sessions['session_id'].isin(val_id)]\n",
        "    test_sessions = sampled_sessions[sampled_sessions['session_id'].isin(test_id)]\n",
        "    \n",
        "    to_pickled_df(args_data, sampled_train=train_sessions)\n",
        "    to_pickled_df(args_data, sampled_val=val_sessions)\n",
        "    to_pickled_df(args_data,sampled_test=test_sessions)\n",
        "\n",
        "    # pad params\n",
        "    STATE_LEN = args_state_len\n",
        "    if \"paper\" == args_format:\n",
        "        PAD = item_size\n",
        "    else:\n",
        "        PAD = 0\n",
        "\n",
        "    print(f'''\n",
        "           Generate Replay Buffer:\n",
        "                Total Session Size : {target_sess_size}\n",
        "                     Train:      {len(train_id)} ids | {len(train_sessions)} actions\n",
        "                     Validation: {len(val_id)} ids | {len(val_sessions)} actions\n",
        "                     Test:       {len(test_id)} ids | {len(test_sessions)} actions\n",
        "                     \n",
        "                Random : {args_random}\n",
        "                Random Seed : {args_seed}\n",
        "                Format : {args_format}\n",
        "    \n",
        "                Total session id number : {session_size}\n",
        "                Total item id number  : {item_size}\n",
        "    ''')\n",
        "\n",
        "    # Generating replay buffer from training data\n",
        "    print(f\"Generating training replay buffer\")\n",
        "\n",
        "    state, len_state, action, is_buy, next_state, len_next_state, is_done = [], [], [], [], [], [], []\n",
        "\n",
        "    groups = train_sessions.groupby(\"session_id\")\n",
        "    ids = train_sessions.session_id.unique()\n",
        "\n",
        "    for id in tqdm(ids):\n",
        "        group = groups.get_group(id)\n",
        "        history = []\n",
        "\n",
        "        # Skip short history interaction\n",
        "        # if group.shape[1] < 3:\n",
        "        #     continue\n",
        "\n",
        "        for index, row in group.iterrows():\n",
        "            s = list(history)\n",
        "            len_state.append(STATE_LEN if len(s) >= STATE_LEN else 1 if len(s) == 0 else len(s))\n",
        "            s = pad_history(s, STATE_LEN, PAD)\n",
        "            a = row['item_id']\n",
        "            state.append(s)\n",
        "            action.append(a)\n",
        "            is_buy.append(row['is_buy'])\n",
        "            history.append(row['item_id'])\n",
        "            next_s = list(history)\n",
        "            len_next_state.append(STATE_LEN if len(next_s) >= STATE_LEN else 1 if len(next_s) == 0 else len(next_s))\n",
        "            next_s = pad_history(next_s, STATE_LEN, PAD)\n",
        "            next_state.append(next_s)\n",
        "            is_done.append(False)\n",
        "        is_done[-1] = True\n",
        "\n",
        "    dic={'state':state,\n",
        "            'len_state':len_state,\n",
        "            'action':action,\n",
        "            'is_buy':is_buy,\n",
        "            'next_state':next_state,\n",
        "            'len_next_states':len_next_state,\n",
        "            'is_done':is_done}\n",
        "\n",
        "\n",
        "    reply_buffer=pd.DataFrame(data=dic)\n",
        "\n",
        "    if \"paper\" == args_format:\n",
        "        reply_buffer.to_pickle(os.path.join(args_data, f'./replay_buffer.df'))\n",
        "        dic = {'state_size': [STATE_LEN], 'item_num': [item_size]}\n",
        "        data_statis = pd.DataFrame(data=dic)\n",
        "        data_statis.to_pickle(os.path.join(args_data, './data_statis.df'))\n",
        "    else:\n",
        "        reply_buffer.to_csv(os.path.join(args_data, f\"./replay_buffer.csv\"), index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpXUD3M4jDDW",
        "outputId": "8e77634b-f907-4471-e7a5-ce6aba3b95a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start reading all transaction data ...\n",
            "Finish reading in 00:01:04\n",
            "\n",
            "Filter and save all valid sampled data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-96208c5697a1>:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sampled_sessions['valid_session'] = sampled_sessions.session_id.map(sampled_sessions.groupby('session_id')['item_id'].size() > 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['timestamp', 'session_id', 'item_id', 'price', 'sales_channel_id'], dtype='object')\n",
            "\n",
            "Start counting popularity ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "212373it [00:10, 20135.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Popularity finished in 00:00:10\n",
            "\n",
            "Start spliting into train, val, test data ...\n",
            "\n",
            "           Generate Replay Buffer:\n",
            "                Total Session Size : 20000\n",
            "                     Train:      14000 ids | 148778 actions\n",
            "                     Validation: 4000 ids | 42438 actions\n",
            "                     Test:       2000 ids | 21157 actions\n",
            "                     \n",
            "                Random : True\n",
            "                Random Seed : 1234\n",
            "                Format : csv\n",
            "    \n",
            "                Total session id number : 1245612\n",
            "                Total item id number  : 96222\n",
            "    \n",
            "Generating training replay buffer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9811/9811 [00:14<00:00, 654.24it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hJbV9QwDuUZK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}